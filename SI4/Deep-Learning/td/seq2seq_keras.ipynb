{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Standardization\n",
    "\n",
    "Written dates can come in many formats, it may be helpful to be able to standardize them for further processing.\n",
    "\n",
    "## In this notebook\n",
    "1. Implement a vanilla Seq2Seq model using a LSTM.\n",
    "2. Implement the dot product variant of Luong attention.\n",
    "3. Plot attention matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = [6, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers, callbacks, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datasets\n",
    "\n",
    "Since it's very easy to generate dates at random, we're going to build the dataset rather than retrieve an existing one.\n",
    "\n",
    "It will be saved for reuse in successive notebook executions.\n",
    "\n",
    "This code is a reworking of one originally proposed by Andrew Ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateGenerator:\n",
    "    \"\"\"\n",
    "    Generate a date string dataset with dates in a random string format and the standard format.\n",
    "    \"\"\"\n",
    "\n",
    "    std_format = '%Y-%m-%d'\n",
    "    formats = [\n",
    "        '%Y-%m-%d',\n",
    "        '%d-%m-%Y',\n",
    "        '%m-%d-%Y',\n",
    "\n",
    "        '%Y/%m/%d',\n",
    "        '%d/%m/%Y',\n",
    "        '%m/%d/%Y',\n",
    "\n",
    "        '%b %d, %Y',\n",
    "        '%b %d, %y',\n",
    "        '%B %d, %Y',\n",
    "\n",
    "        #'%A %B %-d, %Y',  # day of week, full month, day, year\n",
    "        #'%A %B %-d %Y',\n",
    "\n",
    "        '%b %d %Y',  # abreviated month, day, year\n",
    "\n",
    "        '%B %d %Y',  # full month, day, year\n",
    "    ]\n",
    "\n",
    "    def random_datetime(self):\n",
    "        \"\"\"Get a random datetime object.\"\"\"\n",
    "        rand_year = random.randint(1900, 2100)\n",
    "        rand_day = random.randint(1, 31)\n",
    "        rand_month = random.randint(1, 12)\n",
    "        try:\n",
    "            return datetime(rand_year, rand_month, rand_day)\n",
    "        except ValueError:\n",
    "            return self.random_datetime()\n",
    "    \n",
    "    def get_sample(self):\n",
    "        \"\"\"\n",
    "        Get a random sample.\n",
    "        Returns the date in (standard_format, random_format, format_index)\n",
    "        \"\"\"\n",
    "        date = self.random_datetime()\n",
    "        rand_format = random.choice(self.formats)\n",
    "        format_index = self.formats.index(rand_format)\n",
    "        std_str = date.strftime(self.std_format)\n",
    "        rand_str = date.strftime(rand_format)\n",
    "        return (std_str, rand_str.lower(), format_index)\n",
    "    \n",
    "    def generate_df(self, n: int):\n",
    "        \"\"\"Generate a df with `n` samples.\"\"\"\n",
    "        return pd.DataFrame(\n",
    "            [self.get_sample() for _ in range(n)],\n",
    "            columns=['output', 'input', 'format']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DATA_PATH = Path('data')\n",
    "if not _DATA_PATH.exists():\n",
    "    _DATA_PATH.mkdir(parents=True)\n",
    "\n",
    "date_generator = DateGenerator()\n",
    "\n",
    "if not (_DATA_PATH / 'train.csv').exists():\n",
    "    train_df = date_generator.generate_df(100000)\n",
    "    train_df.to_csv(_DATA_PATH / 'train.csv', index=False)\n",
    "else:\n",
    "    train_df = pd.read_csv(_DATA_PATH / 'train.csv')\n",
    "\n",
    "if not (_DATA_PATH / 'val.csv').exists():\n",
    "    val_df = date_generator.generate_df(10000)\n",
    "    val_df.to_csv(_DATA_PATH / 'val.csv', index=False)\n",
    "else:\n",
    "    val_df = pd.read_csv(_DATA_PATH / 'val.csv')\n",
    "\n",
    "if not (_DATA_PATH / 'test.csv').exists():\n",
    "    test_df = date_generator.generate_df(10000)\n",
    "    test_df.to_csv(_DATA_PATH / 'test.csv', index=False)\n",
    "else:\n",
    "    test_df = pd.read_csv(_DATA_PATH / 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>input</th>\n",
       "      <th>format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1914-09-21</td>\n",
       "      <td>21-09-1914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2078-04-01</td>\n",
       "      <td>01-04-2078</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998-07-04</td>\n",
       "      <td>07/04/1998</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002-01-13</td>\n",
       "      <td>01/13/2002</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1994-09-02</td>\n",
       "      <td>02/09/1994</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       output       input  format\n",
       "0  1914-09-21  21-09-1914       1\n",
       "1  2078-04-01  01-04-2078       1\n",
       "2  1998-07-04  07/04/1998       5\n",
       "3  2002-01-13  01/13/2002       5\n",
       "4  1994-09-02  02/09/1994       4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best possible score 0.9034\n"
     ]
    }
   ],
   "source": [
    "# If the format is 7, then the year is only 2 digits long. This makes it difficult to predict a 4-digit year\n",
    "print(\"best possible score\", (len(test_df)-np.sum(test_df['format']==7))/len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and create character -> int mapping\n",
    "\n",
    "The letter -> id transformation will be performed using a dictionary that we need to build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab():\n",
    "    tokens = ['<pad>', '<unk>', '<sos>', '<eos>'] + sorted(list(set(list((''.join(train_df['input'])+''.join(train_df['output'])).lower()))))\n",
    "    print(\"Tokens:\",tokens)\n",
    "    tok_to_int = {c: i for i, c in enumerate(tokens)}\n",
    "    int_to_tok = [c for c, i in tok_to_int.items()]\n",
    "    assert tok_to_int['<pad>'] == 0 # by convention\n",
    "    assert tok_to_int['<unk>'] == 1 # by convention\n",
    "    assert tok_to_int['<sos>'] == 2 # start of sequence\n",
    "    assert tok_to_int['<eos>'] == 3 # end of sequence\n",
    "    return tok_to_int, int_to_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['<pad>', '<unk>', '<sos>', '<eos>', ' ', ',', '-', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'y']\n",
      "\n",
      "Taille du vocabulaire : 39\n"
     ]
    }
   ],
   "source": [
    "tok_to_int, int_to_tok = create_vocab()\n",
    "_VOCAB_SIZE = len(tok_to_int)\n",
    "print(\"\\nTaille du vocabulaire :\", _VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a dataset\n",
    "\n",
    "Input (X): \n",
    "\n",
    "    encoder input: Noisy date text.i.e.\n",
    "                   `Saturday December 23, 1834`\n",
    "    \n",
    "    decoder input: (the teacher) the standard date text without the last timestep,\n",
    "                   include <sos> excludes <eos>. \n",
    "                   This is only needed when we are training with \"teacher forcing\".\n",
    "                   Otherwise the decoder makes predictions on its own without knowing\n",
    "                   what it the correct answer was for the previous timestep.\n",
    "                   i.e. <sos>1834-12-23\n",
    "    \n",
    "Output (y): Standard date text without the first timestep, include <eos>, excludes <sos>\n",
    "                   i.e. i.e. 1834-12-23<eos>\n",
    "    \n",
    "The following dataset code is specific to pytorch and may seem a bit convoluted. It just maps an integer index to a single training sample, as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for encoder input the padding is 'pre', <sos> and <eos> are not added\n",
    "# for output <eos> is added at the end, followed by padding if necessary \n",
    "def define_dataset(data, max_len, padding='pre'):\n",
    "    result = np.zeros((len(data), max_len), dtype=int)\n",
    "    for i, d in enumerate(data):\n",
    "        d = [tok_to_int[i] for i in list(d)]\n",
    "        assert len(d) <= max_len\n",
    "        if padding=='post': # for y\n",
    "            result[i,:(len(d)+1)] = d + [tok_to_int['<eos>']]\n",
    "        else: # for X\n",
    "            result[i,-len(d):] = d\n",
    "    assert np.max(result) < _VOCAB_SIZE\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple d'entrée encodeur : [ 0  0  0  0  0  0  0  0 10  9  6  8 17  6  9 17  9 12]\n"
     ]
    }
   ],
   "source": [
    "# Define X\n",
    "_INPUT_LENGTH = max(len(d) for d in train_df['input'])\n",
    "X_train = define_dataset(train_df['input'], _INPUT_LENGTH, padding='pre')\n",
    "X_val = define_dataset(val_df['input'], _INPUT_LENGTH, padding='pre')\n",
    "X_test = define_dataset(test_df['input'], _INPUT_LENGTH, padding='pre')\n",
    "X_train[0]\n",
    "print(\"Exemple d'entrée encodeur :\", X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de sortie  décodeur : [ 9 17  9 12  6  8 17  6 10  9  3  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# Define y\n",
    "_OUTPUT_LENGTH = max(len(d) for d in train_df['output']) + 1\n",
    "y_train = define_dataset(train_df['output'], _INPUT_LENGTH, padding='post')\n",
    "y_val = define_dataset(val_df['output'], _INPUT_LENGTH, padding='post')\n",
    "y_test = define_dataset(test_df['output'], _INPUT_LENGTH, padding='post')\n",
    "print(\"Exemple de sortie  décodeur :\", y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_teacher_data(y_data, max_len):\n",
    "    teacher_data = np.zeros((len(y_data), max_len), dtype=int)\n",
    "\n",
    "    for i, seq in enumerate(y_data):\n",
    "        seq_without_eos = seq[seq > 0][:-1] \n",
    "        teacher_data[i, :len(seq_without_eos) + 1] = [tok_to_int['<sos>']] + list(seq_without_eos)\n",
    "    \n",
    "    return teacher_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple d'entrée teacher forcing : [ 2  9 17  9 12  6  8 17  6 10  9]\n",
      "<sos>1914-09-21\n"
     ]
    }
   ],
   "source": [
    "# Define decoder input (teacher) : add sos at the top + copy the output minus the last element\n",
    "teacher_train = create_teacher_data(y_train, _OUTPUT_LENGTH)\n",
    "teacher_val = create_teacher_data(y_val, _OUTPUT_LENGTH)\n",
    "teacher_test = create_teacher_data(y_test, _OUTPUT_LENGTH)\n",
    "print(\"Exemple d'entrée teacher forcing :\", teacher_train[0])\n",
    "\n",
    "output_tokens = [int_to_tok[i] for i in teacher_train[0]]\n",
    "\n",
    "output_text = ''.join(output_tokens)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vanilla RNN/LSTM\n",
    "![Luong Figure 1.](img/luong_seq2seq.png)\n",
    "\n",
    "### Implementation:\n",
    "1. Encode the input text, you will get 1 vector for each step in the input.\n",
    "2. Keep the RNN's hidden state from the last time step, call this $h_t$. In the picture, these are the two arrows between the blue and red blocks.\n",
    "3. Initialize the decoder's hidden state with $h_t$.\n",
    "4. Let the decoder make predictions one step at a time. This will modify $h_t$.\n",
    "5. Feed the last prediction and $h_t$ as the next step of the decoder.\n",
    "6. Stop when you reach the \"<eos\\>\" marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model config\n",
    "# this is a pretty small network, any bigger and the model can easy solve this problem.\n",
    "# I am purposefully trying to make it difficult for the vanilla model to \"solve\" this problem.\n",
    "\n",
    "_EMBEDDING_SIZE = 32\n",
    "_RNN_SIZE = 32\n",
    "_DROPOUT_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving models\n",
    "_MODEL_PATH = Path('models')\n",
    "if not _MODEL_PATH.exists():\n",
    "    _MODEL_PATH.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Seq2Seq Model\n",
    "\n",
    "Keras implementation of a Seq2Seq model using an embedding layer followed a RNN layer.\n",
    "\n",
    "I have commented on the shape of the output tensor for each line.\n",
    "\n",
    "```\n",
    "B = batch size\n",
    "T = time steps\n",
    "e = embedding dimension\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder():\n",
    "    inputs = layers.Input(shape=(_INPUT_LENGTH,), name=\"encInput\")\n",
    "    h = layers.Embedding(input_dim=_VOCAB_SIZE, output_dim=_EMBEDDING_SIZE)(inputs)\n",
    "    encoder_outputs, memory_state, carry_state = layers.LSTM(_RNN_SIZE, return_sequences=True, return_state=True,\n",
    "                                                             dropout=_DROPOUT_RATE, recurrent_dropout=_DROPOUT_RATE)(h)\n",
    "    encoder_context = [memory_state, carry_state]\n",
    "    return models.Model(inputs, [encoder_outputs, encoder_context], name=\"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = build_encoder()\n",
    "utils.plot_model(encoder,\n",
    "                 show_shapes=True,\n",
    "                #show_dtype=True,\n",
    "                show_layer_names=True,\n",
    "                rankdir=\"TB\",\n",
    "                expand_nested=True,\n",
    "                dpi=200,\n",
    "                show_layer_activations=True,\n",
    "                show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder():\n",
    "    teacher_inputs = layers.Input(shape=(_OUTPUT_LENGTH,), name=\"decInput\")\n",
    "    context_input = [layers.Input(shape=(_RNN_SIZE,), name=\"ctxInputH\"),\n",
    "                     layers.Input(shape=(_RNN_SIZE,), name=\"ctxInputC\")]\n",
    "\n",
    "    # Meme principe que l'encoder\n",
    "    # Une couche d'embedding\n",
    "    ... = layers.Embedding(input_dim=..., output_dim=...)(...)\n",
    "    # Une couche de RNN dont l'état initial est context_input\n",
    "    decoder_outputs, memory_state, carry_state = layers.LSTM(_RNN_SIZE, return_sequences=..., return_state=...,\n",
    "                                                            dropout=_DROPOUT_RATE, recurrent_dropout=_DROPOUT_RATE)(..., initial_state=...)\n",
    "    context = [memory_state, carry_state]\n",
    "    # Une couche Dense pour prédire la bonne lettre\n",
    "    outputs = layers.Dense(..., activation='softmax')(...)\n",
    "    return models.Model([teacher_inputs, context_input], [outputs, context], name=\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = build_decoder()\n",
    "utils.plot_model(decoder,\n",
    "                 show_shapes=True,\n",
    "                #show_dtype=True,\n",
    "                show_layer_names=True,\n",
    "                rankdir=\"TB\",\n",
    "                expand_nested=True,\n",
    "                dpi=200,\n",
    "                show_layer_activations=True,\n",
    "                show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_seq2seq(encoder, decoder):\n",
    "    # Il est maintenant possible de construire le Seq2Seq en connectant l'encodeur au decodeur\n",
    "    \n",
    "    enc_inputs = layers.Input(shape=(_INPUT_LENGTH,), name=\"encInput\")\n",
    "    teacher_inputs = layers.Input(shape=(_OUTPUT_LENGTH,), name=\"teacherInput\")\n",
    "\n",
    "    # Appel de l'encoder\n",
    "    ..., ... = encoder(...)\n",
    "\n",
    "    # Appel du décodeur\n",
    "    ..., ... = decoder([..., ...])\n",
    "\n",
    "    # Construction du modèle \n",
    "    # L'entrée est l'entrée de l'encodeur et l'entrée du décodeur (le teacher)\n",
    "    # La sortie est la sequence de sortie du décodeur\n",
    "    return models.Model([enc_inputs, teacher_inputs], [...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seq2seq = build_seq2seq(encoder, decoder)\n",
    "utils.plot_model(seq2seq,\n",
    "                 show_shapes=True,\n",
    "                #show_dtype=True,\n",
    "                show_layer_names=True,\n",
    "                rankdir=\"TB\",\n",
    "                expand_nested=True,\n",
    "                dpi=200,\n",
    "                show_layer_activations=True,\n",
    "                show_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train teacher seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement du modèle seq2seq - rien de particulier si ce n'est que c'est un peu long\n",
    "# Profittez-en pour réfléchir à la manière dont vous allez utiliser ce modèle pour prédire\n",
    "if not ((_MODEL_PATH / \"encoder.keras\").exists() and (_MODEL_PATH / \"decoder.keras\").exists() and (_MODEL_PATH / \"seq2seq.keras\").exists()):\n",
    "    seq2seq.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    hist = seq2seq.fit([X_train, teacher_train], y_train,\n",
    "          validation_data=([X_val, teacher_val], y_val),\n",
    "          batch_size=128,\n",
    "          epochs=500,\n",
    "          callbacks=[callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)],\n",
    "    )\n",
    "    pd.DataFrame.from_dict(hist.history).plot()\n",
    "    encoder.save(_MODEL_PATH / \"encoder.keras\")\n",
    "    decoder.save(_MODEL_PATH / \"decoder.keras\")\n",
    "    seq2seq.save(_MODEL_PATH / \"seq2seq.keras\")\n",
    "else:\n",
    "    encoder = models.load_model(_MODEL_PATH / \"encoder.keras\")\n",
    "    decoder = models.load_model(_MODEL_PATH / \"decoder.keras\")\n",
    "    seq2seq = models.load_model(_MODEL_PATH / \"seq2seq.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Predict with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour transformer la liste d'id en une chaine de caractères\n",
    "def decode(pred):\n",
    "    return ''.join([int_to_tok[int(id)] for id in pred if id not in [0,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(seq2seq, X):    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(seq2seq, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification sur une valeur\n",
    "decode(X_test[0]), decode(y_test[0]), decode(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut évaluer le modèle sur sa capacité à reconnaitre chaque caractère\n",
    "# même si cela à peu de sens\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test.ravel(), pred.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il est bien plus pertinent de regarder si la chaine prédite est égale à la chaine à prédire\n",
    "def score(y_true, y_pred):\n",
    "    # renvoie le pourcentage de date correctement prédite\n",
    "    return np.round(100*(len(y_true)-inc)/len(y_true),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre resultat devrait être entre 70 et 80 %\n",
    "score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Seq2Seq w/ Luong Attention\n",
    "\n",
    "\n",
    "\n",
    "Les attentions sont décrites dans le papier suivant (https://arxiv.org/pdf/1508.04025.pdf) qui est une mise en œuvre de l'attention de Luong. Il existe de nombreux modèles d'attention mais celle-ci est la plus simple et est suffisante pour ce TP.\n",
    "\n",
    "![Luong Figure 2.](img/luong_attn.png)\n",
    "\n",
    "### Justification du mécanisme d'Attention\n",
    "* Un seul vecteur de longueur fixe ne peut pas stocker une quantité infinie d'information.\n",
    "* Le défi du Seq2Seq est que beaucoup d'informations sont perdues entre l'encodeur et le décodeur. En effet, un seul vecteur de longueur fixe (le dernier état caché de l'encodeur) est transmis au décodeur. Cela oblige le décodeur à traduire le message codé avec des informations dégradées.\n",
    "* Une meilleure approche consisterait à transmettre l'intégralité de la séquence codée à chacune des étapes du décodeur, ce qui garantit l'absence de perte d'informations. En pratique, il s'agit de la séquence d'états cachés du codeur qui pourrait être accessible à chacune des étapes du décodeur.\n",
    "* Le mécanisme d'attention permet de « se concentrer » sur un élément particulier du contexte à partir d'une requête. Ce mécanisme est couramment utilisé dans la traduction automatique neuronale (NMT). Dans la traduction automatique neuronale, le « contexte » est défini par le codeur.\n",
    "\n",
    "### Mise en oeuvre\n",
    "1. Encodage du texte d'entrée pour récupérer 1 vecteur pour chaque étape de l'entrée. Ce tenseur que l'on appelera value aura la forme (None, _INPUT_LENGTH, _RNN_SIZE)\n",
    "2. De la même manière on récupère 1 vecteur pour chaque étape du décodeur. Ce tenseur que l'on appelera query aura la forme (None, _OUTPUT_LENGTH, _RNN_SIZE)\n",
    "3. Pour chacune des paires (query, value), nous allons effectuer le produit vectoriel des deux vecteurs pour obtenir un tenseur de la forme (None, _OUTPUT_LENGTH, _INPUTS_LENGTH). Il s'agit des scores bruts.\n",
    "4. L'étape suivante est très simple puisqu'il s'agit de normaliser cette matrice à l'aide d'un 'softmax' pour obtenir les scores d'attention\n",
    "5. Il s'agit maintenant de faire le produit vectoriel de chacune des valeurs issues de l'encodeur (value) par son score correspondant et ce, pour chacune des étapes du décodeur. On obtient un tenseur de la forme (None, _OUTPUT_LENGTH, _RNN_SIZE)\n",
    "6. On peut maintenant concaténer ce tenseur avec la sortie query du décodeur pour prédire le token de sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'encodeur n'est pas modifié"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémentation des étape 3, 4 et 5\n",
    "def attention(query, value, mode='dot'):\n",
    "    # 1. value vient de l'encodeur - shape : None, _OUTPUT_LENGTH, _RNN_SIZE\n",
    "    # 2. query vient du décodeur - shape : None, _INPUT_LENGTH, _RNN_SIZE\n",
    "    \n",
    "    # 3. la fonction mathématique dot (produit vectoriel) permet de calculer une similarité entre 2 vecteurs - attention à bien utiliser les bon 'axes'\n",
    "    attention = layers.dot(...)\n",
    "    assert attention.shape == (None, _OUTPUT_LENGTH, _INPUT_LENGTH)\n",
    "\n",
    "    # 4. normalisation des scores: uniquement un couche d'Activation='softmax'\n",
    "    attention_scores = layers....\n",
    "    assert attention_scores.shape == (None, _OUTPUT_LENGTH, _INPUT_LENGTH)\n",
    "\n",
    "    # 5. calcul du contexte d'attention - attention à bien utiliser les bon 'axes'\n",
    "    attention_context = layers.dot(...)\n",
    "    assert attention_context.shape == (None, _OUTPUT_LENGTH, _RNN_SIZE)\n",
    "    \n",
    "    return attention_context, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le decodeur est un peu modifié pour inclure\n",
    "# --> le calcul de l'attention par appel de la fonction précédente\n",
    "# --> et effectuer la concaténation entre le vecteur de contexte et la sortie de la couche RNN du décodeur (étape 6)\n",
    "def build_decoder_attention():\n",
    "    teacher_inputs = layers.Input(shape=(_OUTPUT_LENGTH,), name=\"decInput\")\n",
    "    context_input = [layers.Input(shape=(_RNN_SIZE,), name=\"ctxInputH\"),\n",
    "                     layers.Input(shape=(_RNN_SIZE,), name=\"ctxInputC\")]\n",
    "    encoder_outputs = layers.Input(shape=(_INPUT_LENGTH, _RNN_SIZE), name=\"EncoderOuputs\")\n",
    "\n",
    "    h = layers.Embedding(...)(...)\n",
    "    decoder_outputs, memory_state, carry_state = layers.LSTM(...)(..., initial_state=...)\n",
    "    decoder_context = [memory_state, carry_state]\n",
    "\n",
    "    attention_context, attention_scores = attention(..., ...)\n",
    "    \n",
    "    decoder_outputs = layers.concatenate([..., ...])\n",
    "    outputs = layers.Dense(..., activation='...')(...)\n",
    "    \n",
    "    return models.Model([encoder_outputs, teacher_inputs, context_input], [outputs, decoder_context, attention_scores], name=\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_attention = build_decoder_attention()\n",
    "utils.plot_model(decoder_attention,\n",
    "                 show_shapes=True,\n",
    "                #show_dtype=True,\n",
    "                show_layer_names=True,\n",
    "                rankdir=\"TB\",\n",
    "                expand_nested=True,\n",
    "                dpi=200,\n",
    "                show_layer_activations=True,\n",
    "                show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reste comme précédemment à assembler l'encodeur est le décodeur\n",
    "def build_seq2seq_attention(encoder, decoder):\n",
    "    enc_inputs = layers.Input(shape=(_INPUT_LENGTH,), name=\"encInput\")\n",
    "    enc_outputs, enc_context = encoder(enc_inputs)\n",
    "    \n",
    "    teacher_inputs = layers.Input(shape=(_OUTPUT_LENGTH,), name=\"teacherInput\")\n",
    "    dec_sequence_output, _, _ = decoder_attention([enc_outputs, teacher_inputs, enc_context])\n",
    "    return models.Model([enc_inputs, teacher_inputs], [dec_sequence_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_attention = build_encoder() \n",
    "seq2seq_attention = build_seq2seq_attention(encoder_attention, decoder_attention)\n",
    "utils.plot_model(seq2seq_attention,\n",
    "                 show_shapes=True,\n",
    "                #show_dtype=True,\n",
    "                show_layer_names=True,\n",
    "                rankdir=\"TB\",\n",
    "                expand_nested=True,\n",
    "                dpi=200,\n",
    "                show_layer_activations=True,\n",
    "                show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On entraine le modèle\n",
    "if not ((_MODEL_PATH / \"encoder_attention.keras\").exists() and (_MODEL_PATH / \"decoder_attention.keras\").exists() and (_MODEL_PATH / \"seq2seq_attention.keras\").exists()):\n",
    "    seq2seq_attention.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    hist = seq2seq_attention.fit([X_train, teacher_train], y_train,\n",
    "          validation_data=([X_val, teacher_val], y_val),\n",
    "          batch_size=128,\n",
    "          epochs=500,\n",
    "          callbacks=[callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)],\n",
    "    )\n",
    "    pd.DataFrame.from_dict(hist.history).plot()\n",
    "    encoder_attention.save(_MODEL_PATH / \"encoder_attention.keras\")\n",
    "    decoder_attention.save(_MODEL_PATH / \"decoder_attention.keras\")\n",
    "    seq2seq_attention.save(_MODEL_PATH / \"seq2seq_attention.keras\")\n",
    "else:\n",
    "    print(\"reload attentionnal model\")\n",
    "    encoder_attention = models.load_model(_MODEL_PATH / \"encoder_attention.keras\")\n",
    "    decoder_attention = models.load_model(_MODEL_PATH / \"decoder_attention.keras\")\n",
    "    seq2seq_attention = models.load_model(_MODEL_PATH / \"seq2seq_attention.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour prédire, on procède par étape comme pour le Seq2Seq mais il faut peut être adapter un peu l'algorithme pour inclure l'attention.\n",
    "def predict_with_attention(seq2seq_attention, X):\n",
    "    ...\n",
    "    return y_pred, attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_attention0(seq2seq_attention, X_test)\n",
    "decode(X_test[0]), decode(y_test[0]), decode(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut maintenant prédire - le score devrait être meilleur et être entre 80 et 90 %\n",
    "score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention matrix\n",
    "\n",
    "Si la fonction de prédiction retourne la matrice d'attention, il est possible de l'afficher pour un exemple donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attention_matrix(id=0):\n",
    "    \"\"\"Inspect an individual sample.\n",
    "    You can specify which dataset to examine with `ds_type`.\n",
    "    \"\"\"\n",
    "    pred, attention_matrix = predict_attention(encoder_attention, decoder_attention, X_test)\n",
    "    plt.imshow(attention_matrix[id], cmap='gray')\n",
    "    plt.xticks(np.arange(len(y_test[id])), labels=[int_to_tok[int(i)] for i in y_test[id]])\n",
    "    plt.yticks(np.arange(len(X_test[id])), labels=[int_to_tok[int(i)] for i in X_test[id]])\n",
    "\n",
    "    print('Input:', decode(X_test[id]))\n",
    "    print('Expected Output:', decode(y_test[id]))\n",
    "    print('Predicted Output:', decode(pred[id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# see what the post training output looks like\n",
    "attn = print_attention_matrix(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
