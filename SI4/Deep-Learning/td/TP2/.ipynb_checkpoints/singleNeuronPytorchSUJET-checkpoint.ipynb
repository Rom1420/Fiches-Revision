{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b30522be",
   "metadata": {},
   "source": [
    "# A single neuron in keras / Pytorch\n",
    "\n",
    "Polytech SI4\n",
    "\n",
    "Diane Lingrand Diane.Lingrand@univ-cotedazur.fr\n",
    "\n",
    "2024-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4986ef0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39m__version__\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "torch.__version__\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "import pandas\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4af221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation \n",
    "from keras.callbacks import EarlyStopping\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb6200-63fe-4d47-8802-be0f7bbf483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cuda if you have a GPU (not necessary for this lab)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87fe4a3",
   "metadata": {},
   "source": [
    "# Solving a simple linear regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a61e6c-07e5-4714-8f71-c31e7dfbf3a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3da8f",
   "metadata": {},
   "source": [
    "For this purpose, we build a synthetic dataset, with some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b7b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200 # number of samples\n",
    "X = torch.rand(n)\n",
    "noise = (torch.rand(n)-0.5)/50\n",
    "y = X/4 + 0.2 + noise # note that the true answer to the regression problem is here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30621f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shapes:\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e2ed92-f567-41bc-adab-bcba34d3a1fc",
   "metadata": {},
   "source": [
    "Let's plot the dataset in order to visualize the amount of noise with the linear problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y, marker='x')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaef406-ae9d-4c1b-a720-02fe92763021",
   "metadata": {},
   "source": [
    "Splitting into train (80%) and test (20%) sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea19803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(0.8*len(X))\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "print(len(X_train),len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9cfd5c-91c2-4368-943e-bc6991af2d85",
   "metadata": {},
   "source": [
    "For this visualisation, we have used different colors and shapes for train and test. Feel free to change the colors and the shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722660ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train,y_train, marker='x', color='blue',label='train')\n",
    "plt.scatter(X_test,y_test, marker='o', color='green', label='test')\n",
    "\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('value')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d83032-5c82-4d49-81fb-661151066be2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##  Using Keras 3 on top of Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4077bac7-3ae4-47c8-a02b-ee6f27617af7",
   "metadata": {},
   "source": [
    "We build here a model composed by a single linear neuron without any activation. The loss function is _mean square error_ **(mse)** and the method for learning is **g**radient **d**escent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(keras.Input(shape=(1,)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44432996-9fa2-455a-9520-a36445e8b762",
   "metadata": {},
   "source": [
    "Let's verify the model and the number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c97d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c875aaf-5377-4019-88e6-a107982538b8",
   "metadata": {},
   "source": [
    "We start the training using 10 epochs and computing the loss function on the whole train set. In you re-run the next cell, 10 new epochs will be added from the previous state of the learned neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ff2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size = len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae192bd-f36b-4893-bcb5-7c083ce2e904",
   "metadata": {},
   "source": [
    "The variable *history* enables to save a trace of the learning for further display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c492fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.title('linear regression with a single neuron')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f282ae-35a0-4cc2-a7f0-d3111e8250f8",
   "metadata": {},
   "source": [
    "Let's display the train set, the test set, the prediction of the train and the test set and the true answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1640122b-1c17-46d2-bdc4-889fd826f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_train = model.predict(X_train)\n",
    "ypred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a65ac-4b3a-4620-832a-92800c61c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, marker='^', color='blue',label='train')\n",
    "plt.scatter(X_test, y_test, marker='o', color='green', label='test')\n",
    "\n",
    "plt.scatter(X_train, ypred_train, marker='x', color='red', label='train predictions')\n",
    "plt.scatter(X_test, ypred_test, marker='.', color='orange', label='test predictions')\n",
    "\n",
    "data = np.linspace(0,1,100)\n",
    "plt.plot(data,0.25*data+0.2, color='yellow', label='true separation')\n",
    "\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('value')\n",
    "plt.title('Data and predictions after few epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9d444-8914-400b-8c2f-6344ca1f4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d8f448-d1fb-49b2-a714-8924761ed12d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6738bab-64f8-4bb0-a356-1113cc01de39",
   "metadata": {},
   "source": [
    "There is space for improvements:\n",
    " - stopping criterion (not only the number of epochs)\n",
    " - batch size\n",
    " - optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb414158-f721-46a9-aedd-2b8f91919ca6",
   "metadata": {},
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df25ff4-2f43-48e3-bd3c-34c803566fc5",
   "metadata": {},
   "source": [
    "Early stopping is a callback that will stop the training if *monitor*, the variable that is monitored, does not improve of at least *min_delta* during *patience* epochs. The fit method is called using a large number of epochs. Since we monitor the loss function on the validation set, we need to define how to split the train set into train and validation using a *validation_split* ratio for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f60361-2346-42ea-88a3-b85fb2ceb12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "ourCallback = EarlyStopping(monitor='val_loss', min_delta=0.000001, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfa30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(X_train, y_train, epochs=2000, batch_size = 160, validation_split=0.2, callbacks=[ourCallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_train = model.predict(X_train)\n",
    "ypred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae227c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, marker='^', color='blue',label='train')\n",
    "plt.scatter(X_test, y_test, marker='o', color='green', label='test')\n",
    "\n",
    "plt.scatter(X_train, ypred_train, marker='x', color='red', label='train predictions')\n",
    "plt.scatter(X_test, ypred_test, marker='.', color='orange', label='test predictions')\n",
    "\n",
    "data = np.linspace(0,1,100)\n",
    "plt.plot(data,0.25*data+0.2, color='yellow', label='true separation)\n",
    "\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('value')\n",
    "plt.title('Data and predictions after learning')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c04c9-c853-46b4-a13f-2236639809a7",
   "metadata": {},
   "source": [
    "<font color=\"green\"><b>Question:</b></font> Using the model's parameters, can you plot the predicted line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a7a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()\n",
    "# your work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d27fb-3383-4198-8fcd-52090e002ecf",
   "metadata": {},
   "source": [
    "<font color=\"green\"><b>Question:</b></font> Experiment the influence of the early stopping parameters for the regression convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b568fe-f8d4-44af-ad3a-552debd22e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dfe2bb-add2-4919-9965-40dee45813c5",
   "metadata": {},
   "source": [
    "### Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc5ef9-cbf4-4ba6-a147-f04bfaa8a5a4",
   "metadata": {},
   "source": [
    "<font color=\"green\"><b>Question:</b></font> Try different batch sizes. What is the influence of the batch size on:\n",
    "- computation time\n",
    "- number of iterations / epochs\n",
    "- performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f9af6-2dd6-45fc-bd56-1cfd87454758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735bc9c5-f356-473a-b435-d68c7a4aded3",
   "metadata": {},
   "source": [
    "<font color=\"green\"><b>Question:</b></font> Display the results of data and predictions after your best learning experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bdfda6-25fa-4492-88c8-7ebca3053289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b81364-8f7b-44b5-b54d-7c37b4f3afe1",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d10af4c-a222-4aa1-affc-23d6735937a1",
   "metadata": {},
   "source": [
    "<font color=\"green\"><b>Question:</b></font> SGD in keras is the simple gradient descent. Examine the <a href='https://keras.io/api/optimizers/sgd/'>API</a> and test potentially interesting parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924f1490-f5a7-47f6-90b2-3799b1336b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca18d51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## The same single neuron but using only Pytorch (for advanced students)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c017b9",
   "metadata": {},
   "source": [
    "### Single neuron as a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6734c89f",
   "metadata": {},
   "source": [
    "We have 2 solutions for implementing a single neuron: \n",
    "\n",
    "The first solution is to implement the linear transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleNeuron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1,requires_grad=True, dtype = torch.float))\n",
    "        self.bias = nn.Parameter(torch.randn(1,requires_grad=True, dtype = torch.float))\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.weights*x+self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d8fe4",
   "metadata": {},
   "source": [
    "The second solution is to use the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\">nn.Linear</a> class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7409c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleNeuron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Linear(in_features=1, out_features=1)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6058141",
   "metadata": {},
   "source": [
    "Let's build one model using a single neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = SingleNeuron()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0668ad35",
   "metadata": {},
   "source": [
    "We have access to the model's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d48c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in myModel.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68715d7f",
   "metadata": {},
   "source": [
    "Let's compute the predictions of the model before learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9796668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): # or inference_mode(): \n",
    "    ypred = myModel(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693fccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train,y_train, marker='x', color='blue',label='train')\n",
    "plt.scatter(X_test,y_test, marker='o', color='green', label='test')\n",
    "plt.scatter(X_test,ypred, marker='.', color='orange', label='test predictions')\n",
    "\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('value')\n",
    "plt.title('Data and predictions before learning')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "error = loss_fn(y_test, ypred)\n",
    "print('mse error = ', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d88072",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ypred.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d802ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4000\n",
    "\n",
    "# Create empty loss lists to track values\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "epoch_count = []\n",
    "\n",
    "lr = 0.002\n",
    "optimizer = torch.optim.SGD(myModel.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    myModel.train() # sets all params that req grad to req grad\n",
    "    \n",
    "    # forward pass\n",
    "    y_pred = myModel(X_train)\n",
    "    # loss computation\n",
    "    loss = loss_fn(y_pred,y_train)\n",
    "    # no gradient accumulation\n",
    "    optimizer.zero_grad()\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    # gradient descent step using optimizer\n",
    "    optimizer.step() # how optim accumulates\n",
    "   \n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass on test data\n",
    "        test_pred = myModel(X_test)\n",
    "    \n",
    "       # 2. Caculate loss on test data\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "       # Print out what's happening\n",
    "\n",
    "        epoch_count.append(epoch)\n",
    "        train_loss_values.append(loss.detach().numpy())\n",
    "        test_loss_values.append(test_loss.detach().numpy())\n",
    "        print(f\"Epoch: {epoch} | MSE Train Loss: {loss} | MSE Test Loss: {test_loss} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cff707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e016b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): # or inference_mode(): \n",
    "    ypred_train = myModel(X_train)\n",
    "    ypred_test = myModel(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = loss_fn(y_test, ypred_test)\n",
    "print('mse error = ', error)\n",
    "print(torch.sum(torch.abs(y_test-ypred_test))/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cc5f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train,y_train, marker='^', color='blue',label='train')\n",
    "plt.scatter(X_test,y_test, marker='o', color='green', label='test')\n",
    "\n",
    "plt.scatter(X_train,ypred_train, marker='x', color='red', label='train predictions')\n",
    "plt.scatter(X_test,ypred_test, marker='.', color='orange', label='test predictions')\n",
    "\n",
    "data = np.linspace(0,1,100)\n",
    "plt.plot(data,0.25*data+0.2, color='yellow', label='truth')\n",
    "\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('value')\n",
    "plt.title('Data and predictions before learning')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in myModel.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbf3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ypred_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb22266c-8b5a-4a40-9490-eca1058e001b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Solving a simple binary linear classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ee510-4b3a-46e8-843a-3aa5c708e99a",
   "metadata": {},
   "source": [
    "## Synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46363c4a-fc7e-4c34-bf22-da5496b2d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200 # number of samples\n",
    "X = torch.rand(n,2)\n",
    "noise = (torch.rand(n)-0.5)/50\n",
    "ySep = X[:,1]-X[:,0]*0.6 - 0.2 + noise # note that the true answer to the regression problem is here!\n",
    "y = np.array([0 if lab < 0 else 1 for lab in ySep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a4d52e-d72e-4b02-b8ef-92897627d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(0.8*len(X))\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "print(len(X_train),len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e50a8-b7d4-4256-971d-9957f30343c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "color = ['blue', 'green']\n",
    "train_colors = [ color[y] for y in y_train]\n",
    "test_colors = [ color[y] for y in y_test]\n",
    "plt.scatter(X_train[:,0],X_train[:,1], marker='x', color=train_colors,label='train')\n",
    "plt.scatter(X_test[:,0],X_test[:,1], marker='o', color=test_colors, label='test')\n",
    "plt.plot([0,1],[0.2,0.8], color='yellow', linewidth=4, label='true')\n",
    "plt.xlabel('$X_0$')\n",
    "plt.ylabel('$X_1$')\n",
    "trainPos =  mlines.Line2D([], [], color='green', linestyle='', marker='x',label='train pos.')\n",
    "trainNeg = mlines.Line2D([], [], color='blue', linestyle='', marker='x',label='train neg.')\n",
    "testPos = mlines.Line2D([], [], color='green', linestyle='', marker='o',label='test pos.')\n",
    "testNeg = mlines.Line2D([], [], color='blue', linestyle='', marker='o',label='test neg.')\n",
    "trueSep = mlines.Line2D([], [], color='yellow', linewidth=4, marker='',label='true sep.')\n",
    "plt.legend(handles=[trainPos,trainNeg,testPos,testNeg,trueSep])\n",
    "plt.savefig('binaryLinearClassDataset.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0283ad42-750d-440b-95be-aacf6ae608f9",
   "metadata": {},
   "source": [
    "## Simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed9478b-78fa-4967-bee3-ffff9301e8d3",
   "metadata": {},
   "source": [
    "We build here a model composed by a single linear neuron with a sigmoid activation. The loss function is the binary cross-entropy (as for the logistic regression) and the method for learning is gradient descent. We add the accuracy metric: it is another metric that will be tracked along the epochs together with the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966de713-5b64-4581-9a2f-15fc682f3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(keras.Input(shape=(2,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e6947c-4c39-4f4e-a300-470014186f17",
   "metadata": {},
   "source": [
    "<font color=\"green\"><b>Question:</b></font> Try to learn this model over few epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec2659a-c766-42ee-a635-1fe91812115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb852e-00f7-4454-ab72-e7949d1081a0",
   "metadata": {},
   "source": [
    "<font color=\"green\"><b>Question:</b></font> Add a callback for early stopping. You can monitor the loss (or val_loss) or the accuracy (or val_accuracy). Try also different mini-batch sizes. Play with these parameters until you are satisfied with the results. For each experiment, you will have to plot the history and displayPlay with these parameters. Play with these parameters. Play with these parameters.  the classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9604bca1-de27-479f-b875-c64b486948ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c53478d-2221-454e-8a1a-0790020e0f23",
   "metadata": {},
   "source": [
    "<font color=\"green\"><b>Question:</b></font> Another way to visualize the results is to plot the prediction. In this example, we use the shape to differentiate the train and test and the colors to differentiate the positive or negative prediction. Feel free to change these parameters. We have also plotted the true separation in yellow and we ask you to fill better the code for the orange line that is the predicted separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b1d9b-78c9-48b1-841b-5aeb8dbc3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work: change these a and b values for the orange line\n",
    "a = 1\n",
    "b = 1\n",
    "color = ['blue', 'green']\n",
    "train_colors = [ color[y] for y in ypred_train]\n",
    "test_colors = [ color[y] for y in ypred_test]\n",
    "plt.scatter(X_train[:,0],X_train[:,1], marker='x', color=train_colors, label='train')\n",
    "plt.scatter(X_test[:,0],X_test[:,1], marker='o', color=test_colors, label='test')\n",
    "plt.plot([0,1],[0.2,0.8], color='yellow', linewidth=4, label='true separation')\n",
    "plt.plot([0,1],[a,b], color='orange', linewidth=4, label='predicted separation')\n",
    "\n",
    "plt.xlabel('$X_0$')\n",
    "plt.ylabel('$X_1$')\n",
    "trainPos =  mlines.Line2D([], [], color='green', linestyle='', marker='x',label='train predict pos.')\n",
    "trainNeg = mlines.Line2D([], [], color='blue', linestyle='', marker='x',label='train predict neg.')\n",
    "testPos = mlines.Line2D([], [], color='green', linestyle='', marker='o',label='test predict pos.')\n",
    "testNeg = mlines.Line2D([], [], color='blue', linestyle='', marker='o',label='test predict neg.')\n",
    "trueSep = mlines.Line2D([], [], color='yellow', linewidth=4, marker='',label='true sep.')\n",
    "predSep = mlines.Line2D([], [], color='orange', linewidth=4, marker='',label='predict. sep.')\n",
    "\n",
    "plt.legend(handles=[trainPos,trainNeg,testPos,testNeg,trueSep, predSep])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
