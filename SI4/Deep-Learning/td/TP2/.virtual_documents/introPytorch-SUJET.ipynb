


import numpy as np
import torch
torch.__version__
import matplotlib.pyplot as plt











a = torch.tensor(2)


print('nb dim = ', a.dim(), '\n shape = ',a.shape, '\n dtype = ', a.dtype, '\n value = ', a.item())








# one solution

b = torch.tensor([2.1, 7.5, 3])


print('nb dim = ', b.dim(), '\n shape = ',b.shape, '\n dtype = ', b.dtype)





bobo = torch.tensor([2,3,4,5])


print('nb dim = ', bobo.dim(), '\n shape = ',bobo.shape, '\n dtype = ', bobo.dtype)





c = torch.tensor([[2, 7, 3],[1,4,9]])


print('nb dim = ', c.dim(), '\n shape = ',c.shape, '\n dtype = ', c.dtype)





# your work
small_grey_pic = torch.tensor([[0.0,0.2,0.4,0.6,0.8],
                              [0.2,0.4,0.6,0.8,1.0],
                              [0.4,0.6,0.8,1.0,0.2],
                              [0.6,0.8,1.0,0.2,0.4],
                              [0.8,1.0,0.2,0.4,0.6]])
plt.imshow(small_grey_pic, cmap= 'gray')
plt.colorbar()
plt.show()





d = torch.tensor([[[2, 7, 3, 6],
                   [1, 4, 9, 11]],
                  [[5, 4, 0, 6],
                   [10, 8, 1, 3]],
                  [[3, 6, 2, 8],
                   [9, 1, 2, 7]]])
print(d)


print('nb dim = ', d.dim(), '\n shape = ',d.shape, '\n dtype = ', d.dtype)





# your work
image = torch.tensor([[[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 1.0, 0.0], [1.0, 0.0, 1.0]],
                      [[0.0, 1.0, 1.0], [0.5, 0.5, 0.5], [0.7, 0.2, 0.3], [0.2, 0.8, 0.5], [0.6, 0.4, 0.8]],
                      [[0.3, 0.7, 0.1], [0.9, 0.3, 0.4], [0.2, 0.6, 0.7], [0.4, 0.8, 0.3], [0.5, 0.5, 0.5]],
                      [[0.0, 1.0, 0.0], [1.0, 1.0, 1.0], [1.0, 0.0, 0.0], [0.6, 0.3, 0.8], [0.1, 0.9, 0.2]],
                      [[0.2, 0.4, 0.6], [0.7, 0.8, 0.1], [0.0, 0.0, 1.0], [1.0, 1.0, 1.0], [1.0, 0.0, 0.0]]], dtype=torch.float32)
plt.imshow(image)
plt.show()





# default device
print("d is stored on device: ", d.device)


# making the code device agnostic
device = "cuda" if torch.cuda.is_available() else "cpu"
device


torch.cuda.device_count()


# move the tensor d to the device
d_gpu = d.to(device)


print("d_gpu is stored on device: ", d_gpu.device)





# run the previous cells








# zeros
z0 = torch.zeros(2,3)
print(z0) 
zc = torch.zeros_like(c)
print(zc)


# ones
z1 = torch.ones(3,4)
print(z1)


# linear
zl = torch.linspace(0,1,5)
print(zl)


# random
zr = torch.randint(0,10,(3,10))
print(zr)


# from and to numpy array
anArray = np.array([1,5,2,7])
aTensor = torch.from_numpy(anArray)
backToNumpy = aTensor.numpy()
print(anArray,aTensor,backToNumpy, sep='\n')





# your work
random_image = torch.rand((20,20,3))
plt.imshow(random_image)


height, width = 20, 20

r_gradient = torch.linspace(0, 1, width).unsqueeze(0).repeat(height, 1)
g_gradient = torch.linspace(0, 1, height).unsqueeze(1).repeat(1, width)
b_gradient = torch.zeros((height, width))

gradientimage = torch.stack([r_gradient, g_gradient, b_gradient], dim=2)

plt.imshow(gradientimage)








# back to first week in SI3 'données numériques'
def f(x):
    return torch.exp(x)

x = torch.linspace(0,10,100)
y = f(x)
plt.plot(x,y)





# this is almost what you have done in SI3 (except that it was using numpy)
# plot exp fn
x = torch.linspace(0,10,100)
y = f(x)
plt.plot(x,y)

# plot tangent at each xi
xi = torch.arange(1,11,2)
yi = f(xi)

# we use the fact that the derivative of exp is exp ( so that df/dx(xi)=f(xi)=yi )
x1 = xi-0.5
x2 = xi+0.5
y1 = yi*(x1-xi) + yi
y2 = yi*(x2-xi) + yi
plt.plot([x1, x2], [y1, y2], marker = 'o')











# plot exp fn
x = torch.linspace(0.0,10.0,100)
y = torch.exp(x)
plt.plot(x,y)

# plot tangent at each xi
xi = torch.arange(1.0,11.0,2.0)
xi.requires_grad_()
yi = torch.exp(xi)

# we explicitely compute the derivatives
yi.backward(torch.FloatTensor(np.onesje veux(len(xi))))
dyi = xi.grad

# drawing as previously
x1 = xi-0.5
x2 = xi+0.5
y1 = dyi*(x1-xi) + yi
y2 = dyi*(x2-xi) + yi
x1np = x1.detach().numpy()
x2np = x2.detach().numpy()
y1np = y1.detach().numpy()
y2np = y2.detach().numpy()
plt.plot([x1np, x2np], [y1np, y2np], marker = 'o')
plt.title('curve and first derivatives')





def f(x):
    return x*x*x*x/4 - 8*x*x*x/3 + 19*x*x/2 - 12*x + 1


x = np.linspace(0,5,100)
y = f(x)
plt.plot(x,y)





# your work
def df(x):
    return x**3 - 8*x**2 + 19*x - 12

x = torch.linspace(0, 10, 100)
y = f(x)
plt.plot(x.numpy(), y.numpy(), label='f(x)')

xi = torch.arange(1.0, 11.0, 2.0)
xi.requires_grad_()
yi = f(xi)

yi.backward(torch.FloatTensor(np.ones(len(xi))))
dyi = xi.grad

for x_i, y_i, dyi in zip(xi, yi, dyi):
    x1 = x_i - 0.5
    x2 = x_i + 0.5
    y1 = dyi * (x1 - x_i) + y_i
    y2 = dyi * (x2 - x_i) + y_i

    x1np = x1.detach().numpy()
    x2np = x2.detach().numpy()
    y1np = y1.detach().numpy()
    y2np = y2.detach().numpy()
    
    plt.plot([x1np, x2np], [y1np, y2np], marker='o', linestyle='-', label=f'Tangent at x={x_i.item()}')









theta = torch.tensor(0.2, requires_grad=True)
alpha = 0.01
limite = 100; epsilon = 0.01; cpt = 1

yi = f(theta)
yi.backward()
dyi = theta.grad 
with torch.no_grad():
    theta -= alpha*dyi
    
while (abs(dyi) > 0.01) and (cpt < limite):
    theta.grad.zero_()
    yi = f(theta)
    yi.backward()
    dyi = theta.grad 
    with torch.no_grad():
        theta -= alpha*dyi
    cpt += 1

print('min of f(x) obtained at iteration %d' %cpt)
print(' with x = %0.2f' %theta.item() , 'and min f(x) = %0.2f'%f(theta).item())





# your work
def f(x):
    return x**4 / 4 - 8*x**3 / 3 + 19*x**2 / 2 - 12*x + 1

theta = torch.tensor(0.2, requires_grad=True)
alpha = 0.01
limite = 100; epsilon = 0.01; cpt = 0

thetas = [theta.item()]
f_values = [f(theta).item()]

yi = f(theta)
yi.backward()
dyi = theta.grad 

with torch.no_grad():
    theta -= alpha*dyi
    
while (abs(dyi) > epsilon ) and (cpt < limite):
    theta.grad.zero_()
    yi = f(theta)
    yi.backward()
    dyi = theta.grad 
    with torch.no_grad():
        theta -= alpha*dyi
    cpt += 1
    thetas.append(theta.item())
    f_values.append(f(theta).item())

print('min of f(x) obtained at iteration %d' % cpt)
print(' with x = %0.2f' % theta.item())
print(' and min f(x) = %0.2f' % f(theta).item())

x = torch.linspace(-2.0, 2.0, 400)
y = f(x)

plt.xlim(min(thetas) - 1, max(thetas) + 1) 
plt.ylim(min(f_values) - 1, max(f_values) + 1)
plt.plot(x.numpy(), y.numpy(), label='f(x)', color='blue')
plt.plot(thetas, [f(torch.tensor(val)).item() for val in thetas], 'ro', label='Optimisation Steps')
plt.grid(True)








# your work






# your work






# your work

