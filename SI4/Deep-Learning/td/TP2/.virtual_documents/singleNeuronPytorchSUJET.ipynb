


!pip install keras


import numpy as np
import torch
from torch import nn
torch.__version__
import matplotlib.pyplot as plt
import matplotlib.lines as mlines
from sklearn.metrics import f1_score, accuracy_score, recall_score, ConfusionMatrixDisplay, confusion_matrix
import pandas
from pandas import DataFrame


import keras
from keras.models import Sequential
from keras.layers import Dense, Activation 
from keras.callbacks import EarlyStopping
keras.__version__


# test cuda if you have a GPU (not necessary for this lab)
torch.cuda.is_available()











n = 200 # number of samples
X = torch.rand(n)
noise = (torch.rand(n)-0.5)/50
y = X/4 + 0.2 + noise # note that the true answer to the regression problem is here!


#checking the shapes:
print(X.shape, y.shape)





plt.scatter(X,y, marker='x')
plt.xlabel('data')
plt.ylabel('value')





train_split = int(0.8*len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]
print(len(X_train),len(X_test))





plt.scatter(X_train,y_train, marker='x', color='blue',label='train')
plt.scatter(X_test,y_test, marker='o', color='green', label='test')

plt.xlabel('data')
plt.ylabel('value')
plt.legend()








model = Sequential()
model.add(keras.Input(shape=(1,)))
model.add(Dense(1))
model.compile(optimizer='sgd', loss='mse')





model.summary()





%%time
history = model.fit(X_train, y_train, epochs=10, batch_size = len(y_train))





print(history.history.keys())
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('linear regression with a single neuron')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend()
plt.show()






ypred_train = model.predict(X_train)
ypred_test = model.predict(X_test)


plt.scatter(X_train, y_train, marker='^', color='blue',label='train')
plt.scatter(X_test, y_test, marker='o', color='green', label='test')

plt.scatter(X_train, ypred_train, marker='x', color='red', label='train predictions')
plt.scatter(X_test, ypred_test, marker='.', color='orange', label='test predictions')

data = np.linspace(0,1,100)
plt.plot(data,0.25*data+0.2, color='yellow', label='true separation')

plt.xlabel('data')
plt.ylabel('value')
plt.title('Data and predictions after few epochs')
plt.legend()


model.get_weights()














from keras.callbacks import EarlyStopping
ourCallback = EarlyStopping(monitor='val_loss', min_delta=0.000001, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=False)


%%time
history = model.fit(X_train, y_train, epochs=2000, batch_size = 160, validation_split=0.2, callbacks=[ourCallback])


ypred_train = model.predict(X_train)
ypred_test = model.predict(X_test)


plt.scatter(X_train, y_train, marker='^', color='blue',label='train')
plt.scatter(X_test, y_test, marker='o', color='green', label='test')

plt.scatter(X_train, ypred_train, marker='x', color='red', label='train predictions')
plt.scatter(X_test, ypred_test, marker='.', color='orange', label='test predictions')

data = np.linspace(0,1,100)
plt.plot(data,0.25*data+0.2, color='yellow', label='true separation)

plt.xlabel('data')
plt.ylabel('value')
plt.title('Data and predictions after learning')
plt.legend()





model.get_weights()
# your work





# your work








# your work





# your work








# your work











class SingleNeuron(nn.Module):
    def __init__(self):
        super().__init__()
        self.weights = nn.Parameter(torch.randn(1,requires_grad=True, dtype = torch.float))
        self.bias = nn.Parameter(torch.randn(1,requires_grad=True, dtype = torch.float))
    def forward(self, x:torch.Tensor) -> torch.Tensor:
        return self.weights*x+self.bias





class SingleNeuron(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear_layer = nn.Linear(in_features=1, out_features=1)
        
    def forward(self, x:torch.Tensor) -> torch.Tensor:
        return self.linear_layer(x)





myModel = SingleNeuron()





for p in myModel.parameters():
    print(p)





with torch.no_grad(): # or inference_mode(): 
    ypred = myModel(X_test)


plt.scatter(X_train,y_train, marker='x', color='blue',label='train')
plt.scatter(X_test,y_test, marker='o', color='green', label='test')
plt.scatter(X_test,ypred, marker='.', color='orange', label='test predictions')

plt.xlabel('data')
plt.ylabel('value')
plt.title('Data and predictions before learning')
plt.legend()


loss_fn = nn.MSELoss()
error = loss_fn(y_test, ypred)
print('mse error = ', error)


print(ypred.shape, y_test.shape)


epochs = 4000

# Create empty loss lists to track values
train_loss_values = []
test_loss_values = []
epoch_count = []

lr = 0.002
optimizer = torch.optim.SGD(myModel.parameters(), lr)


# training loop

for epoch in range(epochs):
    myModel.train() # sets all params that req grad to req grad
    
    # forward pass
    y_pred = myModel(X_train)
    # loss computation
    loss = loss_fn(y_pred,y_train)
    # no gradient accumulation
    optimizer.zero_grad()
    # backprop
    loss.backward()
    # gradient descent step using optimizer
    optimizer.step() # how optim accumulates
   
    with torch.inference_mode():
        # 1. Forward pass on test data
        test_pred = myModel(X_test)
    
       # 2. Caculate loss on test data
        test_loss = loss_fn(test_pred, y_test)
       # Print out what's happening

        epoch_count.append(epoch)
        train_loss_values.append(loss.detach().numpy())
        test_loss_values.append(test_loss.detach().numpy())
        print(f"Epoch: {epoch} | MSE Train Loss: {loss} | MSE Test Loss: {test_loss} ")


# Plot the loss curves
plt.plot(epoch_count, train_loss_values, label="Train loss")
plt.plot(epoch_count, test_loss_values, label="Test loss")
plt.title("Training and test loss curves")
plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.legend();


with torch.no_grad(): # or inference_mode(): 
    ypred_train = myModel(X_train)
    ypred_test = myModel(X_test)


error = loss_fn(y_test, ypred_test)
print('mse error = ', error)
print(torch.sum(torch.abs(y_test-ypred_test))/n)


plt.scatter(X_train,y_train, marker='^', color='blue',label='train')
plt.scatter(X_test,y_test, marker='o', color='green', label='test')

plt.scatter(X_train,ypred_train, marker='x', color='red', label='train predictions')
plt.scatter(X_test,ypred_test, marker='.', color='orange', label='test predictions')

data = np.linspace(0,1,100)
plt.plot(data,0.25*data+0.2, color='yellow', label='truth')

plt.xlabel('data')
plt.ylabel('value')
plt.title('Data and predictions before learning')
plt.legend()


for p in myModel.parameters():
    print(p)


print(ypred_train.shape)








n = 200 # number of samples
X = torch.rand(n,2)
noise = (torch.rand(n)-0.5)/50
ySep = X[:,1]-X[:,0]*0.6 - 0.2 + noise # note that the true answer to the regression problem is here!
y = np.array([0 if lab < 0 else 1 for lab in ySep])


train_split = int(0.8*len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]
print(len(X_train),len(X_test))


color = ['blue', 'green']
train_colors = [ color[y] for y in y_train]
test_colors = [ color[y] for y in y_test]
plt.scatter(X_train[:,0],X_train[:,1], marker='x', color=train_colors,label='train')
plt.scatter(X_test[:,0],X_test[:,1], marker='o', color=test_colors, label='test')
plt.plot([0,1],[0.2,0.8], color='yellow', linewidth=4, label='true')
plt.xlabel('$X_0$')
plt.ylabel('$X_1$')
trainPos =  mlines.Line2D([], [], color='green', linestyle='', marker='x',label='train pos.')
trainNeg = mlines.Line2D([], [], color='blue', linestyle='', marker='x',label='train neg.')
testPos = mlines.Line2D([], [], color='green', linestyle='', marker='o',label='test pos.')
testNeg = mlines.Line2D([], [], color='blue', linestyle='', marker='o',label='test neg.')
trueSep = mlines.Line2D([], [], color='yellow', linewidth=4, marker='',label='true sep.')
plt.legend(handles=[trainPos,trainNeg,testPos,testNeg,trueSep])
plt.savefig('binaryLinearClassDataset.pdf')








model = Sequential()
model.add(keras.Input(shape=(2,)))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])





# your work





# your work





# your work: change these a and b values for the orange line
a = 1
b = 1
color = ['blue', 'green']
train_colors = [ color[y] for y in ypred_train]
test_colors = [ color[y] for y in ypred_test]
plt.scatter(X_train[:,0],X_train[:,1], marker='x', color=train_colors, label='train')
plt.scatter(X_test[:,0],X_test[:,1], marker='o', color=test_colors, label='test')
plt.plot([0,1],[0.2,0.8], color='yellow', linewidth=4, label='true separation')
plt.plot([0,1],[a,b], color='orange', linewidth=4, label='predicted separation')

plt.xlabel('$X_0$')
plt.ylabel('$X_1$')
trainPos =  mlines.Line2D([], [], color='green', linestyle='', marker='x',label='train predict pos.')
trainNeg = mlines.Line2D([], [], color='blue', linestyle='', marker='x',label='train predict neg.')
testPos = mlines.Line2D([], [], color='green', linestyle='', marker='o',label='test predict pos.')
testNeg = mlines.Line2D([], [], color='blue', linestyle='', marker='o',label='test predict neg.')
trueSep = mlines.Line2D([], [], color='yellow', linewidth=4, marker='',label='true sep.')
predSep = mlines.Line2D([], [], color='orange', linewidth=4, marker='',label='predict. sep.')

plt.legend(handles=[trainPos,trainNeg,testPos,testNeg,trueSep, predSep])
