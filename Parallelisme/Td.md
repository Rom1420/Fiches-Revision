## Synchronisation (cuda.synchronize())
En CUDA, le calcul sur le GPU peut √™tre asynchrone, ce qui signifie que les threads peuvent continuer √† travailler sans attendre que d'autres threads aient termin√© leur travail. 

La fonction ``cuda.synchronize()`` est utilis√©e pour forcer la synchronisation, c'est-√†-dire **attendre que toutes les op√©rations** pr√©c√©dentes sur le GPU soient **termin√©es** avant de continuer l'ex√©cution du code sur le CPU. 

```python
    start = timer()
    computeInt[blocksPerGrid, threadsPerBlock](a, n)  # Lancement du kernel sur le GPU
    cuda.synchronize()  # Attendre que toutes les op√©rations GPU se terminent
    dt = timer() - start  # Mesurer le temps √©coul√©
```

 ```python
    # Cr√©ation des √©v√©nements
    start_event = cuda.event()
    end_event = cuda.event()

    # Enregistrement du d√©but de l'ex√©cution
    start_event.record()

    # Lancement du kernel sur le GPU
    computeInt[blocksPerGrid, threadsPerBlock](a, n)

    # Enregistrement de la fin de l'ex√©cution
    end_event.record()

    # Attendre que l'ex√©cution du GPU soit termin√©e avant de calculer l'√©cart
    cuda.synchronize()

    # Calcul du temps √©coul√© entre les deux √©v√©nements
    elapsed_time = cuda.event_elapsed_time(start_event, end_event)

    print(f"Temps d'ex√©cution (en millisecondes) : {elapsed_time} ms")
```


"""
###  Memory transfer
In this exercise you will instantiate an array on the host, send it to the device. A kernel will write in the array and finally the host will get the data back.

Instantiate an array of size 32 on the host and fill it with 0
Write the code to send the array to the device
Write a kernel where each thread write its local ID in the corresponding array cell. For example, thread with local ID 4 will do array[4]=4
Write the code to copy back the array after the execution of the kernel and print its content
Call your kernel with a grid size of 1 and and 32 threads. Is it working?"""

```python
from numba import cuda
import numpy as np


@cuda.jit
def kernel(array):
    global_id = cuda.grid(1)
    array[global_id] = global_id


def run(size):
    threadsPerBlock = size
    blocksPerGrid = 1
    print("threadsPerBlock ", threadsPerBlock)
    print("blocksPerGrid", blocksPerGrid)
    A = np.zeros(size, dtype=np.int32)
    d_A = cuda.to_device(A)
    kernel[blocksPerGrid, threadsPerBlock](d_A)
    cuda.synchronize()
    A = d_A.copy_to_host()
    print(A)


run(32)
```


### Memory transfer
In this exercise you will instantiate an array on the host, send it to the device. A kernel will write in the array and finally the host will get the data back.

Instantiate an array of size 32 on the host and fill it with 0
Write the code to send the array to the device
Write a kernel where each thread write its local ID in the corresponding array cell. For example, thread with local ID 4 will do array[4]=4
Write the code to copy back the array after the execution of the kernel and print its content
Call your kernel with a grid size of 1 and and 32 threads. Is it working?"""

```python
from numba import cuda
import numpy as np


@cuda.jit
def kernel(array):
    global_id = cuda.grid(1)
    if global_id < array.shape[0]:
        array[global_id] = global_id


def run(size, threads):
    threadsPerBlock = threads
    blocksPerGrid = 1
    print("threadsPerBlock ", threadsPerBlock)
    print("blocksPerGrid", blocksPerGrid)
    A = np.zeros(size, dtype=np.int32)
    d_A = cuda.to_device(A)
    kernel[blocksPerGrid, threadsPerBlock](d_A)
    cuda.synchronize()
    A = d_A.copy_to_host()
    print(A)


run(30, 32)
```

### Larger array
``` python
from numba import cuda
import numpy as np


@cuda.jit
def kernel(array):
    global_id = cuda.grid(1)
    if global_id < array.shape[0]:
        array[global_id] = global_id


def run(size, threads):
    threadsPerBlock = threads
    blocksPerGrid = (size + threadsPerBlock - 1) // threadsPerBlock
    print("threadsPerBlock ", threadsPerBlock)
    print("blocksPerGrid", blocksPerGrid)
    A = np.zeros(size, dtype=np.int32)
    d_A = cuda.to_device(A)
    kernel[blocksPerGrid, threadsPerBlock](d_A)
    cuda.synchronize()
    A = d_A.copy_to_host()
    print(A)


run(50, 32)
```

### Transformation image

```python
from numba import cuda
import numba as nb
import numpy as np
import time
from PIL import Image
from timeit import default_timer as timer
import math
import sys

@cuda.jit
def RGBToBWKernel(source, destination, offset):
    height = source.shape[1]
    width = source.shape[0]
    #offset =8 
    x,y = cuda.grid(2)
    if (x<width and y<height) :
        r_x= (x+offset)%width
        r_y= (y+offset)%height
        # ( (0.3 * R) + (0.59 * G) + (0.11 * B) )
        destination[r_x,r_y]=np.uint8(0.3*source[r_x,r_y,0]+0.59*source[r_x,r_y,1]+0.11*source[r_x,r_y,2])



def gpu_run(imagetab,threadsperblock, blockspergrid ):
        print("Sending image to device ", end=" ")
        #start = timer()
        s_image = cuda.to_device(imagetab)
        d_image = cuda.device_array((imagetab.shape[0],imagetab.shape[1],3),dtype = np.uint8)
        
        
        for off in range(33,1,-1):
            print("--- offset ---", off)
            runs = 6
            result =np.zeros(runs, dtype=np.float32)
            for i in range(runs):
                print("Executing kernel  ", end=" ")
                start = timer()
                RGBToBWKernel[blockspergrid, threadsperblock](s_image, d_image,off) 
                cuda.synchronize()
                dt = timer() - start
                print(" ", dt, " s")
                result[i]=dt
                
            # dt = timer() - start
            #print(" ", dt, " s")
            print("Average :", threadsperblock[0],off, np.average(result[1:]))
        
        output=d_image.copy_to_host()
        return output
        
def cpu_run(source):
    output=np.empty_like(source)
    height = source.shape[1]
    width = source.shape[0]
    print("Executing on CPU   ", end=" ")
    start = timer()
    for x in range(width):
        for y in range(height):
            output[x,y]=np.uint8(0.3*source[x,y,0]+0.59*source[x,y,1]+0.11*source[x,y,2])
    dt = timer() - start
    print(" ", dt, " s")        
    return output
            
def compute_threads_and_blocks(imagetab):
    threadsperblock = (8,8)
    if len(sys.argv) ==4:
        threadsperblock=(int(sys.argv[3]),int(sys.argv[3]))
    width, height = imagetab.shape[:2]
    blockspergrid_x = math.ceil(width / threadsperblock[0])
    blockspergrid_y = math.ceil(height / threadsperblock[1])
    blockspergrid = (blockspergrid_x, blockspergrid_y)
    print("Thread blocks ", threadsperblock)
    print("Grid ", blockspergrid)
    return threadsperblock,blockspergrid
    
    
if len(sys.argv) < 3:
    print("Usage: ", sys.argv[0]," <inputFile> <outputFile>")
    quit(-1)
    
inputFile = sys.argv[1]
outputFile=sys.argv[2]


    im = Image.open(inputFile)
    imagetab = np.array(im)

    threadsperblock,blockspergrid=compute_threads_and_blocks(imagetab)
    output=gpu_run(imagetab, threadsperblock, blockspergrid)
    # output=cpu_run(imagetab)
    m = Image.fromarray(output) #.convert('RGB')
    m.save(outputFile)
```

# Transformation d'image avec CUDA et Numba

Ce script Python utilise **CUDA avec Numba** pour appliquer une **transformation d'image** sur GPU.  
Il convertit une image couleur **RGB en niveaux de gris**, tout en appliquant un **d√©calage (`offset`)** sur les pixels.  

---

## **1. Chargement de l'image**
- L'image est charg√©e avec **PIL** et convertie en un tableau NumPy.
- **Dimensions de l‚Äôimage** : `width √ó height √ó 3` (3 canaux pour R, G, B).

---

## **2. D√©finition du kernel CUDA (`RGBToBWKernel`)**
- Ex√©cut√© en parall√®le sur **GPU**.
- Convertit chaque pixel en **niveau de gris** en appliquant la formule :  
  \[
  \text{gray} = 0.3 \times R + 0.59 \times G + 0.11 \times B
  \]
- Chaque thread traite un pixel, mais avec un **d√©calage (`offset`)** pour repositionner l'√©criture.

---

## **3. Ex√©cution sur le GPU (`gpu_run`)**
- Envoie l‚Äôimage sur le **GPU**.
- Lance le kernel CUDA avec des **grilles et blocs de threads**.
- **Plusieurs essais** sont effectu√©s avec **diff√©rents offsets** pour mesurer la performance.
- La synchronisation CUDA garantit que chaque it√©ration est bien termin√©e avant de mesurer le temps d‚Äôex√©cution.

---

## **4. Ex√©cution sur le CPU (`cpu_run`)**
- M√™me transformation en niveaux de gris mais en **boucle for classique** (beaucoup plus lent que CUDA).
- Permet de comparer les performances avec l‚Äôex√©cution GPU.

---

## **5. Calcul du nombre de threads et blocs (`compute_threads_and_blocks`)**
- D√©finit une grille de threads adapt√©e √† la taille de l‚Äôimage.
- Par d√©faut, les **blocs** font **8√ó8** threads.
- Ajuste le nombre total de **blocs** pour couvrir toute l‚Äôimage.

---

## **6. Ex√©cution principale**
- **Lit l'image d'entr√©e**.
- **Calcule les blocs et threads** pour CUDA.
- **Ex√©cute le kernel sur GPU**.
- **Convertit le r√©sultat en image et l‚Äôenregistre**.

---

## **üîπ Points importants**
- **Pourquoi CUDA ?** ‚Üí Beaucoup plus rapide qu'une boucle `for` sur CPU.
- **Pourquoi un `offset` ?** ‚Üí Peut am√©liorer l‚Äôacc√®s m√©moire, mais ici son effet exact reste √† analyser.
- **Pourquoi tester plusieurs fois ?** ‚Üí Pour avoir une **moyenne des performances** et √©viter les fluctuations.

Ce script est un bon exemple d'optimisation GPU en utilisant **CUDA avec Numba** pour acc√©l√©rer le traitement d‚Äôimage. üöÄ
