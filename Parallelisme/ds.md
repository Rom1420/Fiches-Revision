# **R√©vision DS**

## **Programmation Concurrente**

### **Threads et Mutex en pthread**
- `pthread_create` : Cr√©e un thread.
- `pthread_join` : Attend la fin d'un thread.
- `pthread_mutex_lock(&lock);` : Verrouille un mutex pour emp√™cher d'autres threads d'y acc√©der.
- `pthread_mutex_unlock(&lock);` : Lib√®re le mutex pour permettre l'acc√®s √† d'autres threads.
- `pthread_mutex_init(&lock, NULL);` : Initialise un mutex.
- `pthread_mutex_destroy(&lock);` : D√©truit un mutex √† la fin.

```c
void *fonction_thread(void *arg) {
    int num = *(int *)arg;
    printf("Thread %d : Hello World !\n", num);
    pthread_exit(NULL);
}

int main() {
    int N = 5;
    pthread_t threads[N];
    int args[N];

    for (int i = 0; i < N; i++) {
        args[i] = i + 1;
        pthread_create(&threads[i], NULL, fonction_thread, &args[i]);
    }

    for (int i = 0; i < N; i++) {
        pthread_join(threads[i], NULL);
    }

    printf("Tous les threads sont termin√©s.\n");
    return 0;
}
```

```c
#define NB_THREADS 4

pthread_mutex_t lock;
int compteur = 0;

void *incrementer(void *arg) {
    for (int i = 0; i < 100000; i++) {
        pthread_mutex_lock(&lock);  // üîí Verrouille le mutex
        compteur++;
        pthread_mutex_unlock(&lock);  // üîì D√©verrouille le mutex
    }
    pthread_exit(NULL);
}

int main() {
    pthread_t threads[NB_THREADS];

    pthread_mutex_init(&lock, NULL);  // Initialisation du mutex

    for (int i = 0; i < NB_THREADS; i++) {
        pthread_create(&threads[i], NULL, incrementer, NULL);
    }

    for (int i = 0; i < NB_THREADS; i++) {
        pthread_join(threads[i], NULL);
    }

    pthread_mutex_destroy(&lock);  // Lib√®re le mutex
    printf("Valeur finale du compteur : %d\n", compteur);
    return 0;
}
```

#### **Deadlock en pthread (Interblocage)**
Un deadlock se produit lorsque deux threads (ou plus) attendent ind√©finiment qu'une ressource se lib√®re.

**Comment l'√©viter ?**
‚úÖ Toujours verrouiller les mutex dans le m√™me ordre.

### **S√©maphore**
Un s√©maphore est une structure de synchronisation permettant de limiter l'acc√®s √† une ressource partag√©e.

#### **Exemple en Java :**
```java
import java.util.concurrent.Semaphore;

class Parking {
    private Semaphore semaphore;

    public Parking(int places) {
        this.semaphore = new Semaphore(places);
    }

    public void entrer(String voiture) {
        try {
            System.out.println(voiture + " essaie d'entrer");
            semaphore.acquire();
            System.out.println(voiture + " est entr√©e üöó (places restantes : " + semaphore.availablePermits() + ")");
            Thread.sleep(2000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        } finally {
            semaphore.release();
            System.out.println(voiture + " sort du parking üèÅ");
        }
    }
}

public class TestSemaphore {
    public static void main(String[] args) {
        Parking parking = new Parking(3);
        for (int i = 1; i <= 5; i++) {
            final String voiture = "Voiture " + i;
            new Thread(() -> parking.entrer(voiture)).start();
        }
    }
}
```

semaphore.acquire() : Une voiture prend une place. Si le parking est plein, elle attend.
semaphore.release() : Une voiture quitte le parking, lib√©rant une place.
Les 5 voitures ne peuvent pas entrer simultan√©ment : seulement 3 places sont disponibles.

üìå **Cas d'usage des s√©maphores :**
‚úÖ Contr√¥le d'acc√®s √† une ressource limit√©e.
‚úÖ Limiter le nombre de threads en ex√©cution simultan√©e.

AtomicInteger garantit que incrementAndGet() est ex√©cut√© sans interf√©rence entre les threads.
Sans AtomicInteger, le r√©sultat serait incorrect et impr√©visible √† cause des interruptions possibles entre valeur++.
üìå Cas d‚Äôusage des Variables Atomiques :
‚úÖ √âviter les conditions de course sans utiliser synchronized ou des locks.
‚úÖ Utilis√© pour des compteurs, indicateurs d'√©tat, etc.

### **Moniteur**

Un monitor est une structure de synchronisation utilis√©e pour √©viter les conditions de course en permettant l'acc√®s exclusif √† une ressource partag√©e. Il fournit :

Un verrou implicite (synchronized en Java)
Des variables conditionnelles (wait(), notify(), notifyAll() en Java)

#### **Exemple en Java :**
```java
private final Object isBufferEmptyCondition = new Object();


@Override
public void put(E data) throws InterruptedException {
	synchronized (isBufferFullCondition) {
		while (count == SIZE) {
			isBufferFullCondition.wait();
		}
	}
	synchronized (isBufferEmptyCondition) {
		queue[tail] = data;
		tail = (tail + 1) % SIZE;
		count++;
		isBufferEmptyCondition.notify();
	}
}
```

#### üìå Explication
Le mot-cl√© synchronized garantit qu'un seul thread peut modifier valeur √† la fois.
Sans synchronized, plusieurs threads pourraient lire et √©crire valeur en m√™me temps, provoquant une condition de course.

- **wait()** : Un thread qui appelle wait() sur un objet lib√®re le verrou et attend qu'un autre thread le r√©veille.
- **notify()**: R√©veille un thread en attente sur le m√™me objet.
- **notifyAll()** : R√©veille tous les threads en attente sur cet objet.

üìå **M√©thodes de synchronisation :**
| M√©thode | Utilisation | Avantages |
|----------|------------|------------|
| Monitor (synchronized) | Exclusion mutuelle + coordination entre threads | Simple √† utiliser, natif en Java |
| Mutex (ReentrantLock) | Verrou exclusif sur une ressource partag√©e | Plus flexible que synchronized |
| S√©maphore | Contr√¥le le nombre de threads acc√©dant √† une ressource | Peut autoriser plusieurs threads simultan√©s |

---

## **Programmation Parall√®le**

### **OpenMP**
OpenMP est une API qui facilite la parall√©lisation en C/C++.

#### **Principales directives :**
- `#pragma omp parallel` : Cr√©e des threads parall√®les.
- `#pragma omp for` : Divise une boucle en plusieurs threads.
- `omp_set_num_threads(nbth)` : D√©finit le nombre de threads.
- `omp_get_num_procs()` : Obtient le nombre de processeurs disponibles.

- `#pragma omp parallel reduction(op : var)` : Effectue une r√©duction parall√®le sur var en appliquant l'op√©rateur op (exemple : +, *, max, min). Chaque thread dispose d'une copie locale de var et une r√©duction est effectu√©e √† la fin de la r√©gion parall√®le.
- `#pragma omp atomic` : Assure une op√©ration atomique sur une variable partag√©e, emp√™chant les conditions de course.
- `#pragma omp critical` : D√©finit une section critique o√π un seul thread √† la fois peut ex√©cuter le code.
- `#pragma omp barrier` : Synchronise tous les threads √† un point donn√© du programme.
- `#pragma omp single` : Sp√©cifie qu‚Äôun seul thread doit ex√©cuter un bloc de code, les autres attendant sa fin.
- `#pragma omp master` : Seul le thread ma√Ætre (thread 0) ex√©cute la section.

---

### **Mod√®le PRAM (Parallel Random Access Machine)**
Le mod√®le PRAM (Parallel Random Access Machine) est un mod√®le th√©orique de calcul parall√®le utilis√© pour analyser et concevoir des algorithmes parall√®les de mani√®re abstraite. Il suppose un ensemble de processeurs (P) travaillant en parall√®le et partageant une m√©moire commune √† acc√®s al√©atoire (RAM).

#### **Variantes du mod√®le PRAM :**
| Mod√®le PRAM | Lecture concurrente (CR) | √âcriture concurrente (CW) |
|-------------|------------------|------------------|
| EREW PRAM  | ‚ùå Non | ‚ùå Non |
| CREW PRAM  | ‚úÖ Oui | ‚ùå Non |
| ERCW PRAM  | ‚ùå Non | ‚úÖ Oui |
| CRCW PRAM  | ‚úÖ Oui | ‚úÖ Oui |

---

Arbitrary CW : Une des valeurs est √©crite al√©atoirement.
Priority CW : Une r√®gle de priorit√© d√©finit quelle valeur est √©crite.
Common CW : Tous les processeurs doivent √©crire la m√™me valeur.

| Mode        | Condition d'√©criture                                           | Exemple d'application                                  |
|-------------|---------------------------------------------------------------|-------------------------------------------------------|
| Arbitrary   | Une seule √©criture r√©ussit, choisie arbitrairement            | Mise √† jour non critique (ex. marquer une m√©moire comme "occup√©e") |
| Consistent  | √âcriture r√©ussie seulement si toutes les valeurs sont identiques | Validation de consensus, protection contre √©criture incoh√©rente |
| Associative | Toutes les √©critures sont combin√©es avec une op√©ration associative | Somme parall√®le, min/max d‚Äôun ensemble               |

## **Introduction aux Ordinateurs Quantiques**

### **Les bases de l‚Äôinformatique quantique**
- **Qubit** : Un qubit peut √™tre en superposition entre 0 et 1.
- **Intrication** : Deux qubits peuvent √™tre li√©s, influen√ßant instantan√©ment l'un l'autre.
- **D√©coh√©rence** : Difficult√© √† maintenir les qubits stables.

### **Pourquoi est-ce important ?**
‚úÖ Les ordinateurs quantiques pourraient r√©soudre certains probl√®mes exponentiellement plus vite.
‚úÖ L'algorithme de Shor pourrait casser la cryptographie classique.

### **Historique et D√©fis**
- **1990s** : Premiers calculateurs quantiques.
- **2009** : Premier processeur quantique √† Yale.
- **D√©fi principal** : Stabilisation des qubits pour √©viter la d√©coh√©rence.

### **Applications possibles**
- **Cryptographie post-quantique**
- **Simulation mol√©culaire pour la m√©decine et la chimie**
- **Optimisation et intelligence artificielle**

üìå **Conclusion** : Les ordinateurs quantiques sont prometteurs mais restent exp√©rimentaux.


### Correction ds

#### 1. Probl√®me √† r√©soudre avec une op√©ration d'√©criture parall√®le
Imaginons un probl√®me o√π plusieurs processeurs veulent ajouter un m√™me type de valeur √† une adresse m√©moire partag√©e. Par exemple, un probl√®me de comptage parall√®le o√π chaque processeur doit incr√©menter un compteur global.

**Op√©ration binaire choisie :**
L‚Äôop√©ration binaire id√©ale dans ce cas serait l'addition. Si plusieurs processeurs veulent incr√©menter la m√™me valeur de compteur simultan√©ment, l'addition permet de cumuler efficacement les valeurs. Chaque processeur additionne simplement sa valeur locale au compteur global.

**Pourquoi l'op√©ration binaire doit-elle √™tre associative ?**
L'associativit√© est cruciale pour garantir que la combinaison des valeurs se fasse ind√©pendamment de l'ordre d'ex√©cution des op√©rations. Par exemple, si plusieurs processeurs tentent d'ajouter une valeur au compteur global, l'ordre dans lequel les valeurs sont combin√©es ne doit pas affecter le r√©sultat final. L'addition est associative car peu importe si on effectue (a + b) + c ou a + (b + c), le r√©sultat final sera le m√™me. Cela permet d'ex√©cuter les √©critures en parall√®le de mani√®re efficace sans se soucier de l'ordre dans lequel les processeurs effectuent leurs ajouts.

#### 2. Reproduire ce comportement avec une EW PRAM
Une EW PRAM (Exclusive Write PRAM) ne permet pas les √©critures concurrentes, c'est-√†-dire qu'√† un moment donn√©, un seul processeur peut √©crire dans la m√©moire. Pour reproduire une op√©ration d‚Äô√©criture parall√®le telle que d√©crite dans le mod√®le PRAM CW (Concurrent Write PRAM) en utilisant une EW PRAM, il faut adopter une strat√©gie pour √©viter les conflits d'√©criture.

**Algorithme parall√®le propos√© :**

L'algorithme parall√®le pourrait √™tre structur√© comme suit :

  - **Phase de collecte des valeurs √† combiner :**
  Chaque processeur propose sa valeur √† combiner (par exemple, sa valeur √† ajouter au compteur global).

  - **Phase de r√©duction parall√®le :**
  Tous les processeurs s'organisent pour appliquer l'op√©ration binaire associative (dans notre exemple, l'addition) sur les valeurs collect√©es. Cela se fait via une r√©duction binaire. Les valeurs sont combin√©es deux √† deux jusqu'√† obtenir une seule valeur r√©sultante.

  - **Phase d'√©criture :**
  Une fois que chaque processeur conna√Æt sa valeur combin√©e, les valeurs combin√©es sont √©crites dans la m√©moire. Comme la EW PRAM ne permet qu'une seule √©criture, les processeurs √©crivent leur valeur dans un ordre sp√©cifique (par exemple, selon un identifiant unique).

**Temps d'ex√©cution parall√®le :**

  - La phase de r√©duction binaire n√©cessite log2(n) √©tapes pour combiner n valeurs, chaque √©tape impliquant une op√©ration de combinatoire associative. Cette phase prend donc un temps parall√®le de O(log n).
  
  - La phase d'√©criture prend O(1) puisque, une fois la valeur combin√©e, l'√©criture est effectu√©e une seule fois.

**Temps total d'ex√©cution parall√®le :**
L'algorithme entier aurait une complexit√© en temps parall√®le de O(log n) pour combiner les valeurs et ensuite O(1) pour l'√©criture. Le temps d'ex√©cution total est donc O(log n).

#### 3. Surco√ªt en temps parall√®le pour une EW PRAM
Lorsqu'on ex√©cute un algorithme con√ßu pour la PRAM CW sur une EW PRAM, il y a un surco√ªt en temps d√ª √† l'absence d'√©critures concurrentes.

**Surco√ªt en temps parall√®le :**
Sur la PRAM CW, plusieurs processeurs peuvent √©crire simultan√©ment, ce qui permet une ex√©cution plus rapide dans certains cas. Sur la EW PRAM, comme les √©critures sont s√©quentielles, il faut utiliser des √©tapes suppl√©mentaires pour combiner les r√©sultats avant l'√©criture, ce qui augmente le temps parall√®le global.

Si l'algorithme sur la PRAM CW a une complexit√© de temps parall√®le de T(n), et en consid√©rant qu'il appartient √† NC (temps poly-logarithmique avec un nombre polynomial de processeurs), le surco√ªt en temps pour une EW PRAM d√©pend principalement du temps n√©cessaire pour g√©rer les conflits d'√©criture.

**Impact sur la complexit√© :**
Pour un algorithme avec une complexit√© PRAM CW de temps parall√®le T(n) = O(log n), il y aura un l√©ger surco√ªt d√ª √† la r√©duction des √©critures parall√®les. Ce surco√ªt peut amener la complexit√© √† O(log^2 n) dans le pire des cas, mais cela reste dans la classe NC, car un algorithme avec un temps parall√®le poly-logarithmique reste dans cette classe. Ainsi, la nouvelle complexit√© en temps reste O(log^2 n), et l'algorithme reste toujours dans NC.

#### 1. Mod√®le fork-join d'OpenMP
Le mod√®le fork-join d'OpenMP signifie que l'ex√©cution commence avec un seul thread, qui se fork (divise) en plusieurs threads lors d'une r√©gion parall√®le. Apr√®s avoir ex√©cut√© le code parall√®le, les threads se synchronisent et join (rejoignent) le thread principal pour continuer l'ex√©cution s√©quentielle.

#### 2. Valeur de j et r√©partition des it√©rations de la boucle for
Valeur de j :
Dans la r√©gion parall√®le, j est initialis√© √† omp_get_thread_num(), qui renvoie l'identifiant du thread courant. Comme il y a 4 threads, j prendra des valeurs de 0 √† 3, une pour chaque thread.

**Instances de j :**
Chaque thread aura sa propre instance de j car la variable j est d√©clar√©e dans la r√©gion parall√®le. Ainsi, chaque thread aura une copie locale de j, et sa valeur d√©pendra de l'identifiant du thread (0, 1, 2, ou 3).

**R√©partition des it√©rations de la boucle for :**
Le pragma #pragma omp for indique que les it√©rations de la boucle for doivent √™tre r√©parties entre les threads. OpenMP utilise une r√©partition statique par d√©faut, o√π chaque thread re√ßoit une portion des it√©rations. Par exemple, avec 4 threads et 8 it√©rations, chaque thread ex√©cutera 2 it√©rations cons√©cutives de la boucle.

**Sans la paire de {} d√©limitant la r√©gion parall√®le :**
Si la paire de {} est omise, le code n'est plus ex√©cut√© en parall√®le. La boucle for devient simplement une boucle s√©quentielle et sera ex√©cut√©e par le thread principal, sans utilisation de #pragma omp parallel pour cr√©er plusieurs threads.

#### 3. Sortie du code avec omp_set_num_threads(4)
Le programme utilise 4 threads, et les it√©rations de la boucle for sont r√©parties entre ces threads de mani√®re statique. Voici le d√©tail de l'ex√©cution :

**R√©partition des it√©rations :**
Le nombre total d'it√©rations est 8. Avec 4 threads, chaque thread obtient 2 it√©rations cons√©cutives.

- Thread 0 re√ßoit les it√©rations 0 et 1.
- Thread 1 re√ßoit les it√©rations 2 et 3.
- Thread 2 re√ßoit les it√©rations 4 et 5.
- Thread 3 re√ßoit les it√©rations 6 et 7.
  
**Affichage des lignes :**
Chaque thread ex√©cutera la boucle for pour ses propres it√©rations et affichera les lignes suivantes :

```mathematica
In FOR: Thread 0 on iteration 0, mod = 0
In FOR: Thread 0 on iteration 1, mod = 0
Thread 0 mod = 0
In FOR: Thread 1 on iteration 2, mod = 2
In FOR: Thread 1 on iteration 3, mod = 3
Thread 1 mod = 3
In FOR: Thread 2 on iteration 4, mod = 8
In FOR: Thread 2 on iteration 5, mod = 10
Thread 2 mod = 10
In FOR: Thread 3 on iteration 6, mod = 18
In FOR: Thread 3 on iteration 7, mod = 21
Thread 3 mod = 21
```

**Ordre des lignes :**
L'ordre des lignes de la boucle printf dans In FOR: Thread ... peut varier car l'ex√©cution des threads n'est pas n√©cessairement ordonn√©e. Les threads peuvent terminer leurs it√©rations √† des moments diff√©rents, donc l'affichage des r√©sultats peut √™tre dans un ordre non d√©termin√©.

**Ligne "Thread X mod = ..." :**
Cette ligne sera affich√©e √† la fin de chaque ex√©cution de la boucle for par chaque thread, et son ordre d√©pendra √©galement de l'ordre d'ex√©cution des threads.

**Conclusion :**
Les lignes sont-elles toujours les m√™mes ? Oui, les lignes d'affichage pour chaque thread sont toujours les m√™mes, mais l'ordre des affichages peut changer d'une ex√©cution √† l'autre.
L'ordre des lignes est-il toujours identique ? Non, l'ordre des lignes dans printf("In FOR: Thread ...") peut varier, car les threads peuvent s'ex√©cuter et afficher leurs r√©sultats dans un ordre non d√©termin√©.






